---
title: "Report - TP2"
author: "Jose Munoz Angulo, Vivien Marcault, Baptiste Rigondaud"
date: "February 26, 2018"
output:
  pdf_document: default
  html_document: default
---

## Question 1

The map is produced with this code:

```{r}
NAm2 = read.table("NAm2.txt", header=TRUE)

names=unique(NAm2$Pop)
npop=length(names)
coord=unique(NAm2[,c("Pop","long","lat")]) #coordinates for each pop
colPalette=rep(c("black","red","cyan","orange","brown","blue","pink","purple","darkgreen"),3)
pch=rep(c(16,15,25),each=9)
plot(coord[,c("long","lat")],pch=pch,col=colPalette,asp=1)
# asp allows to have the correct ratio between  axis  longitude and latitude
# Then the map is not deformed  
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=2,lwd=2)
library(maps);map("world",add=T)
```

The map is created by extracting all unique indigenous population along with its latitude and longitude. By the means of the maps library we draw the map with the populations in its location, using 9 different colors and three different symbols.

## Question 2

The linear regression can be computed like this:

```{r, eval=FALSE}
NAaux = NAm2[,-c(1:7)]
y <- lm(long ~ ., data = NAaux)
summary(y)
```

However R can not invert the matrix to solve te linear regression problem, because of the size of the matrix. Thus, it explains why the linear regression summary is full of Not A Number. In order to have a correct model, we could take a subset of the predictors using this command:

```{r, eval=FALSE}
prcomp(rank=2, NAm2[,-c(1:8)])
```

In the following questions, we are going to use PCA.

## Question 3

### a)
This method consist into diagonalizing the covariance matrix in order to find its maximum eigenvalues. These values correspond to the components of maximum variance. Therefore if we make a projection of the data on the factorspace generated by the corresponding eigenvectors, we preserve the maximum information that we can have from the original data in this dimension.

### b)
The PCA on the genetic data can be computed like this:
```{r}
pcaNAm2 = prcomp(NAm2[,-c(1:8)], scale=TRUE)
```
We use the scale parameter in order to normalize the variances of each component, because each values for each component can be scaled differently, causing variances to be very different.

### c)
```{r}
caxes=c(1,2)
plot(pcaNAm2$x[,caxes],col="white")
for (i in 1:npop) {
  lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],
        type="p",col=colPalette[i],pch=pch[i])
}
legend("bottomleft",legend=names,col=colPalette,lty=-
         1,pch=pch,cex=.75,ncol=3,lwd=2)
```

The graph maps the weight of each genetic samples regarding the two first components of the PCA. We can see that these two first components gathers all the members of a population, but gathers also all the population around the same area.
The 5th and th 6th axes separate more the populations (Pima, Karitiana) compared to the two first axes.

### d)
In order to have the percentages, we use the command:
```{r}
prop = summary(pcaNAm2)$importance[2,]
prop["PC1"] + prop["PC2"]
```

The first two principal components capture $3.4$% of the variance.
The number of principle components that we want to keep in order to explain the data depends on the percentage of the variance we want to explain, and the importance that we give on taking more components to explain a bigger part of the variance.
The script below computes the number of components we would keep to explain the data, using a function that computes the total variance explained by taking $i$ components, minus the percentage of components used to explain the data ($\frac{i}{494}$), $\forall i \in \{1, ..., 494\}$.

```{r}
res = rep(0, 494)
x=0
for (i in 1:494) {
  x = x + prop[i]
  res[i] = x - i/494
}

plot(c(1:494), res, type="l")
i = which(res == max(res))
i
```

Using this function, we would use $190$ principal components.

## Question 4

### a)

```{r}
latlongaxes=c(1:250)
lmlat <- lm(NAm2$lat~pcaNAm2$x[,latlongaxes])
lmlong <- lm(NAm2$long~pcaNAm2$x[,latlongaxes])
plot(lmlong$fitted.values,lmlat$fitted.values,col="white", asp=1, main = "Predicted positions using 250 PC axes", xlab = "Longitude", ylab = "Latitude")
for (i in 1:npop) {
  lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],lmlat$fitted.values[which(NAm2[,3]==names[i])],type="p",col=colPalette[i],pch=pch[i]
  )
}
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=2,lwd=2)
map("world",add=T)
```

Comparing with the first map, we can see that the population are well located and well centered around the expected position based on training data. But the map illustrate too optimistically the ability to locate people according to their genetic material. In fact we tested our model with the training data.

### b)

```{r}
m1 <- matrix(c(lmlong$fitted.values, lmlat$fitted.values), ncol = 2, byrow = FALSE)
m2 <- matrix(c(NAm2$long, NAm2$lat), ncol=2)
dist <- fields::rdist.earth(m1, m2, miles=FALSE)
mea <- mean(diag(dist))
mea
```

We can see that the population are predicted with a mean error of arround $476$km.

## Question 5

### a)

The cross validation method consists into dividing our data in a training set and a test set. The optimisation of the prediction method is made with the training set and its validity is mesured with the training set. It is possible to do it once or multiple times by dividing the data in different ways.
This method is interesting to build a predictive model because it allows you to test your model on data no used to train it. Indeed test your model with the training data is really thwart.

### b)

To predict position using a cross vaidation we used this script in this case:
```{r}
# Dividing the data in 10 sets
set = sample(c(rep(0:9, each=49),1,2,3,4))

naxes = 4

#Creation of data frame for the prediction 
predictedCoord = data.frame(matrix(ncol=2, nrow=length(set)))
colnames(predictedCoord) <- c("longitude", "latitude")

#Preselection of data
pcalong=data.frame(cbind(long=NAm2[,c("long")],pcaNAm2$x[,c(1:naxes)]))
pcalat=data.frame(cbind(lat=NAm2[,c("lat")],pcaNAm2$x[,c(1:naxes)]))
pca=pcaNAm2$x[,c(1:naxes)]

# For each case
for (i in 0:9) {
  lmlat <- lm(lat~ ., dat=pcalat, subset=set != i)
  lmlong <- lm(long~ ., data = subset(pcalong, set != i))
  predictedCoord[set == i,"latitude"] <- predict(lmlat, data.frame(subset(pca, set == i)))
  predictedCoord[set == i,"longitude"] <- predict(lmlong, data.frame(subset(pca, set == i)))
}
m1 <- matrix(c(predictedCoord[,"longitude"], predictedCoord[,"latitude"]), ncol=2)
m2 <- matrix(c(NAm2$long, NAm2$lat), ncol=2)
dist <- fields::rdist.earth(m1, m2, miles=FALSE)
error <- mean(diag(dist))

```
The result for this prediction using only 4 principal componants is an error of 1967 km.

### c)

Let's plot the evolution of error according to the number of principal componants used.
```{r include=FALSE}
cross.validation <- function(naxes) {
  set = sample(c(rep(0:9, each=49),1,2,3,4))
  #b1
  predictedCoord = data.frame(matrix(ncol=2, nrow=length(set)))
  colnames(predictedCoord) <- c("longitude", "latitude")
  
  pcalong=data.frame(cbind(long=NAm2[,c("long")],pcaNAm2$x[,c(1:naxes)]))
  pcalat=data.frame(cbind(lat=NAm2[,c("lat")],pcaNAm2$x[,c(1:naxes)]))
  pca=pcaNAm2$x[,c(1:naxes)]
  
  for (i in 0:9) {
    lmlat <- lm(lat~ ., dat=pcalat, subset=set != i)
    lmlong <- lm(long~ ., data = subset(pcalong, set != i))
    predictedCoord[set == i,"latitude"] <- predict(lmlat, data.frame(subset(pca, set == i)))
    predictedCoord[set == i,"longitude"] <- predict(lmlong, data.frame(subset(pca, set == i)))
  }
  m1 <- matrix(c(predictedCoord[,"longitude"], predictedCoord[,"latitude"]), ncol=2)
  m2 <- matrix(c(NAm2$long, NAm2$lat), ncol=2)
  dist <- fields::rdist.earth(m1, m2, miles=FALSE)
  return(mean(diag(dist)))
}
```
```{r}
res <- NULL
for (i in seq(2,440,by=10)) {
  res <- c(res, cross.validation(i))
}
plot(seq(2,440,by=10), res, type="l", main = "Prediction error according to the number of PC used", sub = "error on validation test of cross-validation with 10 sets", xlab = "number of PCs", ylab = "error (km)")
```

### d)
To use, approximatly, the first 50 principal components seems to be the best choice according to this cross validation. Indeed it minimizes the error on the validation set. With this model we can achieve an error of around 1000km. It is the double of the error we had for the question 4.c but it seems natural. When we tested the model on its training data we couldn't really measure the efficiency of our model. But now, we separate the training data from the validation data we obtain a better approximation of our model's efficiency.

```{r echo=FALSE}
naxes = 50
set = sample(c(rep(0:9, each=49),1,2,3,4))
#b1
predictedCoord = data.frame(matrix(ncol=2, nrow=length(set)))
colnames(predictedCoord) <- c("longitude", "latitude")

pcalong=data.frame(cbind(long=NAm2[,c("long")],pcaNAm2$x[,c(1:naxes)]))
pcalat=data.frame(cbind(lat=NAm2[,c("lat")],pcaNAm2$x[,c(1:naxes)]))
pca=pcaNAm2$x[,c(1:naxes)]

for (i in 0:9) {
  lmlat <- lm(lat~ ., dat=pcalat, subset=set != i)
  lmlong <- lm(long~ ., data = subset(pcalong, set != i))
  predictedCoord[set == i,"latitude"] <- predict(lmlat, data.frame(subset(pca, set == i)))
  predictedCoord[set == i,"longitude"] <- predict(lmlong, data.frame(subset(pca, set == i)))
}
plot(predictedCoord,col="white", asp=1, main = "Predicted positions using 50 PC axes", sub = "Using cross-vaidation sets", xlab = "Longitude", ylab = "Latitude")
for (i in 1:npop) {
  lines(predictedCoord[,"longitude"][which(NAm2[,3]==names[i])],predictedCoord[,"latitude"][which(NAm2[,3]==names[i])],type="p",col=colPalette[i],pch=pch[i]
  )
}
map("world",add=T)
```


## Question 6

The linear model approach is a good starting point, but there are some cases in which it doesn't work correctly. Sometimes, the data that needs to be analyzed contains many more attributes than observations. This causes the linear model to break, as it cannot be created properly. For these problems we can use different methods that will yield results that might be good but also some that might be lacking in quality.

The first approach is to only take a subset of the parameters can be used to create a linear model, but it is a naive method and the model is not accurate enough to be useful. A method that can help create a better model is Principal Components Analysis or PCA. This technique helps us reduce the number of parameters separating them on components that can be used in a model as if it were the parameters. PCA improves the quality of predictors compared to the previous approach.

Creating a 2D representation of the result after PCA separates a part of the population but most of it is grouped together. This makes sense given that the first two components, those with the higher explained variance, only explain 3.4% of it. So, more components are needed to have a better model but trying simultaneously to keep the number of components to a minimum. This number is the one that have the best accumulated explained variance - number of components used ratio, i.e., 190 components. Using 250 components, the generated model creates that predicts well the location of the population according to the genetic material, with a mean error of approximately 476km.

