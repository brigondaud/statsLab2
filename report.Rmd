---
title: "Report - TP2"
author: "Jose Munoz Angulo, Vivien Marcault, Baptiste Rigondaud"
date: "February 26, 2018"
output: html_document
---

## Question 1

The map is produced with this code:

```{r}
NAm2 = read.table("NAm2.txt", header=TRUE)

names=unique(NAm2$Pop)
npop=length(names)
coord=unique(NAm2[,c("Pop","long","lat")]) #coordinates for each pop
colPalette=rep(c("black","red","cyan","orange","brown","blue","pink","purple","darkgreen"),3)
pch=rep(c(16,15,25),each=9)
plot(coord[,c("long","lat")],pch=pch,col=colPalette,asp=1)
# asp allows to have the correct ratio between  axis  longitude and latitude
# Then the map is not deformed  
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=2,lwd=2)
library(maps);map("world",add=T)
```

The map is created by extracting all unique indigenous population along with its latitude and longitude. By the means of the maps library we draw the map with the populations in its location, using 9 different colors and three different symbols.

## Question 2

The linear regression can be computed like this:

```{r, eval=FALSE}
NAaux = NAm2[,-c(1:7)]
y <- lm(long ~ ., data = NAaux)
summary(y)
```

However R can not invert the matrix to solve te linear regression problem, because of the size of the matrix. Thus, it explains why the linear regression summary is full of Not A Number. In order to have a correct model, we could take a subset of the predictors using this command:

```{r, eval=FALSE}
prcomp(rank=2, NAm2[,-c(1:8)])
```

In the following questions, we are going to use PCA.

## Question 3

### a)
This method consist into diagonalizing the covariance matrix in order to find its maximum eigenvalues. These values correspond to the components of maximum variance. Therefore if we make a projection of the data on the factorspace generated by the corresponding eigenvectors, we preserve the maximum information that we can have from the original data in this dimension.

### b)
The PCA on the genetic data can be computed like this:
```{r}
pcaNAm2 = prcomp(NAm2[,-c(1:8)], scale=TRUE)
```
We use the scale parameter in order to normalize the variances of each component, because each values for each component can be scaled differently, causing variances to be very different.

### c)
```{r}
caxes=c(1,2)
plot(pcaNAm2$x[,caxes],col="white")
for (i in 1:npop) {
  lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],
        type="p",col=colPalette[i],pch=pch[i])
}
legend("bottomleft",legend=names,col=colPalette,lty=-
         1,pch=pch,cex=.75,ncol=3,lwd=2)
```

The graph maps the weight of each genetic samples regarding the two first components of the PCA. We can see that these two first components gathers all the members of a population, but gathers also all the population around the same area.
The 5th and th 6th axes separate more the populations (Pima, Karitiana) compared to the two first axes.

### d)
In order to have the percentages, we use the command:
```{r}
prop = summary(pcaNAm2)$importance[2,]
prop["PC1"] + prop["PC2"]
```

The first two principal components capture $3.4$% of the variance.
The number of principle components that we want to keep in order to explain the data depends on the percentage of the variance we want to explain, and the importance that we give on taking more components to explain a bigger part of the variance.
The script below computes the number of components we would keep to explain the data, using a function that computes the total variance explained by taking $i$ components, minus the percentage of components used to explain the data ($\frac{i}{494}$), $\forall i \in \{1, ..., 494\}$.

```{r}
res = rep(0, 494)
x=0
for (i in 1:494) {
  x = x + prop[i]
  res[i] = x - i/494
}

plot(c(1:494), res, type="l")
i = which(res == max(res))
i
```

Using this function, we would use $190$ principal components.

## Question 4

### a)

```{r}
latlongaxes=c(1:250)
lmlat <- lm(NAm2$lat~pcaNAm2$x[,latlongaxes])
lmlong <- lm(NAm2$long~pcaNAm2$x[,latlongaxes])
plot(lmlong$fitted.values,lmlat$fitted.values,col="white", asp=1)
for (i in 1:npop) {
  lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],lmlat$fitted.values[which(NAm2[,3]==names[i])],type="p",col=colPalette[i],pch=pch[i]
  )
}
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=2,lwd=2)
map("world",add=T)
```

Comparing with the first map, we can see that the population are well located and well centered around the expected position based on training data. But the map illustrate too optimistically the ability to locate people according to their genetic material. In fact we tested our model with the training data.

### b)

```{r}
m1 <- matrix(c(lmlong$fitted.values, lmlat$fitted.values), ncol = 2, byrow = FALSE)
m2 <- matrix(c(NAm2$long, NAm2$lat), ncol=2)
dist <- fields::rdist.earth(m1, m2, miles=FALSE)
mea <- mean(diag(dist))
mea
```

We can see that the population are predicted with a mean error of arround $476$km.

## Question 5

### a)

The cross validation method consists into dividing our data in a training set and a test set. The optimisation of the prediction method is made with the training set and its validity is mesured with the training set. It is possible to do it once or multiple times by dividing the data in different ways.
This method is interesting to build a predictive model because it allows you to test your model on data no used to train it. Indeed test your model with the training data is really thwart.

### b)

The index of subset can be created like this:

```{r}
set = sample(c(rep(0:9, each=49),1,2,3,4))

```

#### 1)

## Question 6

The linear model approach is a good starting point, but there are some cases in which it doesn't work correctly. Sometimes, the data that needs to be analyzed contains many more attributes than observations. This causes the linear model to break, as it cannot be created properly. For these problems we can use different methods that will yield results that might be good but also some that might be lacking in quality.

The first approach is to only take a subset of the parameters can be used to create a linear model, but it is a naive method and the model is not accurate enough to be useful. A method that can help create a better model is Principal Components Analysis or PCA. This technique helps us reduce the number of parameters separating them on components that can be used in a model as if it were the parameters. PCA improves the quality of predictors compared to the previous approach.

Creating a 2D representation of the result after PCA separates a part of the population but most of it is grouped together. This makes sense given that the first two components, those with the higher explained variance, only explain 3.4% of it. So, more components are needed to have a better model but trying simultaneously to keep the number of components to a minimum. This number is the one that have the best accumulated explained variance - number of components used ratio, i.e., 190 components. Using 250 components, the generated model creates that predicts well the location of the population according to the genetic material, with a mean error of approximately 476km.

